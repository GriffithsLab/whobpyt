{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "momi_2025_example.ipynb\n\nOriginal file is located at\n    https://colab.research.google.com/drive/1dD_XqTVleoW8VgH_T_y5SrFxBNOu_ZRz\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Commented out IPython magic to ensure Python compatibility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!pip install mne\n!pip install nilearn\n\n\n#@title Install dependencies\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# @title whobpyt Package\n\n\nimport os\nimport sys\nimport json\n\n#sys.path.append('../whobpyt')\n\n\n\ndata = {\"username\":\"claires03\",\"key\":\"ee39084a8974336d9fff7e1ced807e64\"}\nkaggle_path = os.path.join('/root/.config', 'kaggle')\nif not os.path.exists(kaggle_path):\n    os.makedirs(kaggle_path)\n    print(f\"Created directory: {kaggle_path}\")\nelse:\n    print(f\"Directory already exists: {kaggle_path}\")\n\nkaggle_file = kaggle_path+ '/kaggle.json'\nprint(kaggle_file)\nwith open(kaggle_file, 'w') as f:\n    json.dump(data, f)\n\nos.chmod(kaggle_file, 0o600)\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add custom module paths\n#from whobpyt.depr.momi2025.euclidean_distance import euclidean_distance\nimport re\nimport math\nimport glob\nimport pickle\nimport requests\n\nimport numpy as np\nimport pandas as pd\nimport scipy\nimport scipy.io\nfrom scipy.signal import find_peaks\nimport sklearn\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport mne\nimport nibabel\nimport nibabel as nib\nfrom nilearn import plotting, surface\nfrom nilearn.image import load_img\n\n# WHOBPYT\nimport torch\nimport whobpyt\nfrom whobpyt.datasets.fetchers import fetch_egmomi2025\nfrom whobpyt.depr.momi2025.jansen_rit import par, Recording, method_arg_type_check, dataloader\n\n\n#from whobpyt.datasets.dataload import dataload\nfrom whobpyt.depr.momi2025.jansen_rit import RNNJANSEN, ParamsJR, CostsJR, Model_fitting\n\n\nimport math\ndef euclidean_distance(coord1, coord2):\n    x1, y1, z1 = coord1[0], coord1[1], coord1[2]\n    x2, y2, z2 = coord2[0], coord2[1], coord2[2]\n    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2 + (z2 - z1)**2)\n\n\"\"\"## Empirical Result\"\"\"\n\ndata_folder = fetch_egmomi2025()\n\n# @title Download Data\n\nstart_time = time.time()\n\n\n\nall_eeg_evoked = np.load(data_folder + '/empirical-data/all_eeg_evoked.npy')\nepo_eeg = mne.read_epochs(data_folder + '/empirical-data/example_epoched.fif', verbose=False)\n\nall_gfma = np.zeros((all_eeg_evoked.shape[0], all_eeg_evoked.shape[2]))\n\nfor ses in range(all_eeg_evoked.shape[0]):\n    all_gfma[ses,:] =  np.std(all_eeg_evoked[ses,:,:],axis=0) #np.mean(np.mean(epo_eeg._data, axis=0),axis=0)\n    #Normalized for the baseline for making comparison\n    all_gfma[ses,:] = np.abs(all_gfma[ses,:] - np.mean(all_gfma[ses, :300]))\n\nwith open(data_folder + '/empirical-data/dist_Schaefer_1000parcels_7net.pkl', 'rb') as handle:\n    dist_Schaefer_1000parcels_7net = pickle.load(handle)\nstim_region = dist_Schaefer_1000parcels_7net['stim_region']\n\n\nnetworks = ['Vis', 'SomMot', 'DorsAttn', 'SalVentAttn', 'Limbic', 'Cont', 'Default']\n# Create a dictionary to store the network indices\nstim_network_indices = {network: [] for network in networks}\nfor i, label in enumerate(stim_region):\n    # Iterate over each network\n    for network in networks:\n        if network in label:\n            stim_network_indices[network].append(i)\n            break\n\nnet_gfma = {}\n\nfor network in networks:\n    net_gfma[network] = all_gfma[stim_network_indices[network]]\n\n\naverages = []\n\nfor key, value in net_gfma.items():\n    average = sum(value) / len(value)\n    averages.append(average)\n\naverages = np.array(averages)\n\n# Download the file from the GitHub URL\nurl = 'https://github.com/Davi1990/DissNet/raw/main/examples/network_colour.xlsx'\ncolour = pd.read_excel(url, header=None)[4]\n\n# Define the desired figure size\nfig = plt.figure(figsize=(20, 6))\n\n# Plot the data\nfor net in range(len(networks)):\n    plt.plot(epo_eeg.times, averages[net, :] - np.mean(averages[net, :300]), colour[net], linewidth=5)\n\n# Display the plot\nplt.show()\n\n# Calculate the mean array as you mentioned\ntime_series = np.mean((averages[:, :] - np.mean(averages[:, :300])), axis=0)\n\n# Find peaks in the time series data\npeaks, _ = find_peaks(-time_series[:700], distance=1)  # Adjust 'distance' parameter as needed\n\npeak_values = time_series[peaks]\n\n# Get the indices of the first 3 peaks in descending order of amplitude\nfirst_3_peak_indices = peaks[np.argsort(peak_values)[::-1][:3]]\n\n# Get the actual values of the first 3 peaks\nfirst_3_peak_amplitudes = peak_values[np.argsort(peak_values)[::-1][:3]]\n\n# Plot the time series and the identified peaks\nplt.figure(figsize=(10, 6))\nplt.plot(time_series, label='Time Series')\nplt.plot(first_3_peak_indices, first_3_peak_amplitudes, 'ro', label='First 3 Peaks')\nplt.legend()\nplt.xlabel('Index')\nplt.ylabel('Value')\nplt.title('Time Series with First 3 Peaks')\nplt.show()\n\n# Assuming you have a 2D array all_gfma with shape (323, 1001)\n# Calculate the mean and standard deviation along the first axis (sessions)\nmean_all_gfma = np.mean(all_gfma, axis=0)\nstd_all_gfma = np.std(all_gfma, axis=0)\n# Calculate the margin of error for the confidence interval\nconfidence_level = 0.95\nz_score = 1.96  # For a 95% confidence interval\nmargin_of_error = z_score * (std_all_gfma / np.sqrt(len(all_gfma)))\n\n# Calculate the upper and lower bounds of the confidence interval\nupper_bound = mean_all_gfma + margin_of_error\nlower_bound = mean_all_gfma - margin_of_error\n\n\nupper_bound =upper_bound - np.mean(upper_bound[:300])\nlower_bound =lower_bound - np.mean(lower_bound[:300])\n\n\nif len(epo_eeg.times) == len(time_series):\n    # Plot the time series and the identified peaks\n    plt.figure(figsize=(20, 6))\n    plt.plot(epo_eeg.times, time_series, label='Time Series')\n    plt.plot(epo_eeg.times[first_3_peak_indices], first_3_peak_amplitudes, 'yo', markersize=1, label='First 3 Peaks')\n    plt.plot(epo_eeg.times, upper_bound,'-r', label='upper')\n    plt.plot(epo_eeg.times, lower_bound,'-g', label='lower')\n    plt.fill_between(epo_eeg.times, upper_bound, lower_bound, color=\"k\", alpha=0.15)  # Use 'epo_eeg.times'\n\n    plt.legend()\n    plt.xlabel('Time (s)')  # Set the x-axis label to 'Time (s)'\n    plt.ylabel('Value')\n    plt.title('Time Series with First 3 Peaks')\n    #plt.savefig('C:/Users/davide_momi/Desktop/peaks.png', dpi=300)\n    plt.show()\nelse:\n    print(\"The lengths of 'epo_eeg.times' and 'time_series' don't match.\")\n\nwindows = 3\nAUC = np.zeros((3,all_gfma.shape[0]))\n\nfirst_3_peak_indices_sorted = sorted(first_3_peak_indices)\nfirst_peak = epo_eeg.times[first_3_peak_indices_sorted[0]]\nsecond_peak = epo_eeg.times[first_3_peak_indices_sorted[1]]\nthird_peak = epo_eeg.times[first_3_peak_indices_sorted[2]]\n\n\nfor ses in range(all_gfma.shape[0]):\n    AUC[0, ses] = np.trapz(all_gfma[ses, np.where(epo_eeg.times==0)[0][0]:np.where(epo_eeg.times==first_peak)[0][0]]\n                           - np.mean(all_gfma[ses, :300]), dx=5)\n    AUC[1, ses] = np.trapz(all_gfma[ses, np.where(epo_eeg.times==first_peak)[0][0]:np.where(epo_eeg.times==second_peak)[0][0]]\n                           - np.mean(all_gfma[ses, :300]), dx=5)\n    AUC[2, ses] = np.trapz(all_gfma[ses, np.where(epo_eeg.times==second_peak)[0][0]:np.where(epo_eeg.times==third_peak)[0][0]]\n                           - np.mean(all_gfma[ses, :300]), dx=5)\n\nAUC[0,:] = AUC[0,:] / (first_3_peak_indices_sorted[0] - 300)\nAUC[1,:] = AUC[1,:] / (first_3_peak_indices_sorted[1] - first_3_peak_indices_sorted[0])\nAUC[2,:] = AUC[2,:] / (first_3_peak_indices_sorted[2] - first_3_peak_indices_sorted[1])\n\n\nnet_AUC = {}\n\nfor network in networks:\n    net_AUC[network] = AUC[:,stim_network_indices[network]]\n\n\nAUC_averages = np.zeros((len(networks), windows))\n\nfor idx, key in enumerate(net_AUC.keys()):\n    AUC_averages[idx, :] = np.mean(net_AUC[key], axis=1)\n\nAUC_averages = AUC_averages *100000\n\n\n# Download the file from the GitHub URL\nurl = 'https://github.com/Davi1990/DissNet/raw/main/examples/network_colour.xlsx'\ncolour = pd.read_excel(url, header=None)[4]\n\n\n\n# Create the figure and subplots\nfig, axs = plt.subplots(1, 3, figsize=(13, 6))  # 2 rows, 1 column\n\n# Plot in the first subplot\naxs[0].bar(range(AUC_averages[:, 1].shape[0]), AUC_averages[:, 0], color=colour)\naxs[0].set_xticks(range(AUC_averages[:, 0].shape[0]))\naxs[0].set_xticklabels(networks, rotation=45)\naxs[0].set_xlabel('Networks')\naxs[0].set_title('Early response 0-' + str(round(first_peak*1000)) + 'ms')\naxs[0].set_ylabel('AUC')\naxs[0].set_ylim(0, 2)  # Adjust the y-axis limits as needed\n\n# Plot in the second subplot (same as the first subplot)\naxs[1].bar(range(AUC_averages[:, 1].shape[0]), AUC_averages[:, 1], color=colour)\naxs[1].set_xticks(range(AUC_averages[:, 0].shape[0]))\naxs[1].set_xticklabels(networks, rotation=45)\naxs[1].set_xlabel('Networks')\naxs[1].set_title('Late response ' + str(round(first_peak*1000)) + '-' + str(round(second_peak*1000)) + 'ms')\naxs[1].set_ylabel('AUC')\naxs[1].set_ylim(0, 2)  # Adjust the y-axis limits as needed\n\n\n# Plot in the second subplot (same as the first subplot)\naxs[2].bar(range(AUC_averages[:, 2].shape[0]), AUC_averages[:, 2], color=colour)\naxs[2].set_xticks(range(AUC_averages[:, 0].shape[0]))\naxs[2].set_xticklabels(networks, rotation=45)\naxs[2].set_xlabel('Networks')\naxs[2].set_title('Late response ' + str(round(second_peak*1000)) + '-' + str(round(third_peak*1000)) + 'ms')\naxs[2].set_ylabel('AUC')\naxs[2].set_ylim(0, 2)  # Adjust the y-axis limits as needed\n\nplt.tight_layout()  # Adjust the spacing between subplots if needed\n\n\nplt.show()\n\nwith open(data_folder + '/empirical-data/all_epo_seeg.pkl', 'rb') as handle:\n    all_epo_seeg = pickle.load(handle)\n\n\nall_gfma = np.zeros((len(list(all_epo_seeg.keys())), epo_eeg._data.shape[2]))\n\nfor ses in range(len(list(all_epo_seeg.keys()))):\n    epo_seeg =all_epo_seeg[list(all_epo_seeg.keys())[ses]]\n    for xx in range(epo_seeg.shape[0]):\n        epo_seeg[xx,:] = epo_seeg[xx,:] - np.mean(epo_seeg[xx,:300])\n\n    all_gfma[ses,:] =  np.std(epo_seeg, axis=0)\n\n\nwith open(data_folder + '/empirical-data/dist_Schaefer_1000parcels_7net.pkl', 'rb') as handle:\n    dist_Schaefer_1000parcels_7net = pickle.load(handle)\nstim_region = dist_Schaefer_1000parcels_7net['stim_region']\n\nnetworks = ['Vis', 'SomMot', 'DorsAttn', 'SalVentAttn', 'Limbic', 'Cont', 'Default']\n# Create a dictionary to store the network indices\nstim_network_indices = {network: [] for network in networks}\nfor i, label in enumerate(stim_region):\n    #if dist_Schaefer_1000parcels_7net['dist'][i] < 7:\n            # Iterate over each network\n            for network in networks:\n                if network in label:\n                    stim_network_indices[network].append(i)\n                    break\n\n\nnet_gfma = {}\n\nfor network in networks:\n    net_gfma[network] = all_gfma[stim_network_indices[network]]\n\n\naverages = []\n\nfor key, value in net_gfma.items():\n    average = sum(value) / len(value)\n    averages.append(average)\n\naverages = np.array(averages)\n\n# Download the file from the GitHub URL\nurl = 'https://github.com/Davi1990/DissNet/raw/main/examples/network_colour.xlsx'\ncolour = pd.read_excel(url, header=None)[4]\n\n# Define the desired figure size\nfig = plt.figure(figsize=(20, 6))\n\n# Plot the data\nfor net in range(len(networks)):\n    plt.plot(epo_eeg.times, averages[net, :] - np.mean(averages[net, :300]), colour[net], linewidth=5)\n\n# Display the plot\n\n\nplt.show()\n\n# Calculate the mean array as you mentioned\ntime_series = np.mean((averages[:, :] - np.mean(averages[:, :300])), axis=0)\n\n# Find peaks in the time series data\npeaks, _ = find_peaks(-time_series, width=15)  # Adjust 'distance' parameter as needed\n\npeak_values = time_series[peaks]\n\n# Get the indices of the first 3 peaks in descending order of amplitude\nfirst_3_peak_indices = peaks[np.argsort(peak_values)[::-1][:3]]\n\nfirst_3_peak_indices = np.array([298, 337, 378, 700])\nfirst_3_peak_amplitudes = time_series[first_3_peak_indices]\n\n\nwindows = 3\nAUC = np.zeros((3,all_gfma.shape[0]))\n\nfirst_peak = epo_eeg.times[first_3_peak_indices[0]]\nsecond_peak = epo_eeg.times[first_3_peak_indices[1]]\nthird_peak = epo_eeg.times[first_3_peak_indices[2]]\nfourth_peak = epo_eeg.times[first_3_peak_indices[3]]\n\n\nfor ses in range(all_gfma.shape[0]):\n    AUC[0, ses] = np.trapz(all_gfma[ses, np.where(epo_eeg.times==first_peak)[0][0]:np.where(epo_eeg.times==second_peak)[0][0]]\n                           - np.mean(all_gfma[ses, :300]), dx=5)\n    AUC[1, ses] = np.trapz(all_gfma[ses, np.where(epo_eeg.times==second_peak)[0][0]:np.where(epo_eeg.times==third_peak)[0][0]]\n                           - np.mean(all_gfma[ses, :300]), dx=5)\n    AUC[2, ses] = np.trapz(all_gfma[ses, np.where(epo_eeg.times==third_peak)[0][0]:np.where(epo_eeg.times==fourth_peak)[0][0]]\n                           - np.mean(all_gfma[ses, :300]), dx=5)\n\n\n\nAUC[0,:] = AUC[0,:] / 33\nAUC[1,:] = AUC[1,:] / 45\nAUC[2,:] = AUC[2,:] / 319\n\nnet_AUC = {}\n\nfor network in networks:\n    net_AUC[network] = AUC[:,stim_network_indices[network]]\n\n\nAUC_averages = np.zeros((len(networks), windows))\n\nfor idx, key in enumerate(net_AUC.keys()):\n    AUC_averages[idx, :] = np.mean(net_AUC[key], axis=1)\n\n\nAUC_averages = AUC_averages*1000\n# AUC_averages = (AUC_averages / np.max(AUC_averages, axis=0)) * 100\n\n\n# Download the file from the GitHub URL\nurl = 'https://github.com/Davi1990/DissNet/raw/main/examples/network_colour.xlsx'\ncolour = pd.read_excel(url, header=None)[4]\n\n\n\n# Create the figure and subplots\nfig, axs = plt.subplots(1, 3, figsize=(13, 6))  # 2 rows, 1 column\n\n# Plot in the first subplot\naxs[0].bar(range(AUC_averages[:, 1].shape[0]), AUC_averages[:, 0], color=colour)\naxs[0].set_xticks(range(AUC_averages[:, 0].shape[0]))\naxs[0].set_xticklabels(networks, rotation=45)\naxs[0].set_xlabel('Networks')\naxs[0].set_title('Early response '+ str(round(first_peak*1000)) + '-' + str(round(second_peak*1000)) + 'ms')\naxs[0].set_ylabel('AUC')\naxs[0].set_ylim(0, 3)  # Adjust the y-axis limits as needed\n\n# Plot in the second subplot (same as the first subplot)\naxs[1].bar(range(AUC_averages[:, 1].shape[0]), AUC_averages[:, 1], color=colour)\naxs[1].set_xticks(range(AUC_averages[:, 0].shape[0]))\naxs[1].set_xticklabels(networks, rotation=45)\naxs[1].set_xlabel('Networks')\naxs[1].set_title('Late response ' + str(round(second_peak*1000)) + '-' + str(round(third_peak*1000)) + 'ms')\naxs[1].set_ylabel('AUC')\naxs[1].set_ylim(0, 3)  # Adjust the y-axis limits as needed\n\n\n# Plot in the second subplot (same as the first subplot)\naxs[2].bar(range(AUC_averages[:, 2].shape[0]), AUC_averages[:, 2], color=colour)\naxs[2].set_xticks(range(AUC_averages[:, 0].shape[0]))\naxs[2].set_xticklabels(networks, rotation=45)\naxs[2].set_xlabel('Networks')\naxs[2].set_title('Late response ' + str(round(third_peak*1000)) + '-' + str(round(fourth_peak*1000)) + 'ms')\naxs[2].set_ylabel('AUC')\naxs[2].set_ylim(0, 3)  # Adjust the y-axis limits as needed\n\nplt.tight_layout()\nplt.show()\n\nend_time = time.time()\nelapsed_time = end_time - start_time\nprint(f\"Elapsed time: {elapsed_time} seconds\")\n\n\"\"\"## Model_fitting\"\"\"\n\n\n# @title Install dependencies\n\nstart_time = time.time()\n\n# Select the session number to use: Please do not change it as we are using subject-specific anatomy\nses2use = 10\n\n# Load the precomputed EEG evoked response data from a file\nall_eeg_evoked = np.load(data_folder + '/empirical-data/all_eeg_evoked.npy')\n\n# Read the epoch data from an MNE-formatted file\nepo_eeg = mne.read_epochs(data_folder + '/empirical-data/example_epoched.fif', verbose=False)\n\n# Compute the average evoked response from the epochs\nevoked = epo_eeg.average()\n\n# Replace the data of the averaged evoked response with data from the selected session\nevoked.data = all_eeg_evoked[ses2use]\n\n# Load additional data from pickle files\nwith open(data_folder + '/empirical-data/all_epo_seeg.pkl', 'rb') as handle:\n    all_epo_seeg = pickle.load(handle)\n\nwith open(data_folder + '/empirical-data/dist_Schaefer_1000parcels_7net.pkl', 'rb') as handle:\n    dist_Schaefer_1000parcels_7net = pickle.load(handle)\n\n# Extract the stimulation region data from the loaded pickle file\nstim_region = dist_Schaefer_1000parcels_7net['stim_region']\n\n# Load Schaefer 200-parcel atlas data from a URL\nurl = 'https://raw.githubusercontent.com/ThomasYeoLab/CBIG/master/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal/Parcellations/MNI/Centroid_coordinates/Schaefer2018_200Parcels_7Networks_order_FSLMNI152_2mm.Centroid_RAS.csv'\natlas = pd.read_csv(url)\n\n# Extract coordinates and ROI labels from the atlas data\ncoords_200 = np.array([atlas['R'], atlas['A'], atlas['S']]).T\nlabel = atlas['ROI Name']\n\n# Remove network names from the ROI labels for clarity\nlabel_stripped_200 = []\n\nfor xx in range(len(label)):\n    label_stripped_200.append(label[xx].replace('7Networks_', ''))\n\n# Load Schaefer 1000-parcel atlas data from a URL\nurl = 'https://raw.githubusercontent.com/ThomasYeoLab/CBIG/master/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal/Parcellations/MNI/Centroid_coordinates/Schaefer2018_1000Parcels_7Networks_order_FSLMNI152_2mm.Centroid_RAS.csv'\natlas = pd.read_csv(url)\n\n# Extract coordinates and ROI labels from the atlas data\ncoords_1000 = np.array([atlas['R'], atlas['A'], atlas['S']]).T\nROI_Name = atlas['ROI Name']\n\n# Remove network names from the ROI labels for clarity\nlabel_stripped_1000 = []\n\nfor xx in range(len(ROI_Name)):\n    label_stripped_1000.append(ROI_Name[xx].replace('7Networks_', ''))\n\n# Find the index of the stimulation region in the list of stripped ROI labels (1000 parcels)\nstim_idx = label_stripped_1000.index(stim_region[ses2use])\n\n# Use the index to get the coordinates of the stimulation region from the 1000-parcel atlas\nstim_coords = coords_1000[stim_idx]\n\n# Extract the network name from the stimulation region label\n# The network name is the part after the underscore in the stimulation region label\nstim_net = stim_region[ses2use].split('_')[1]\n\nimport math\n# Initialize an empty list to store distances\ndistances = []\n\n# Iterate over each coordinate in the 200-parcel atlas\nfor xx in range(coords_200.shape[0]):\n    # Compute the Euclidean distance between the current coordinate and the stimulation coordinates\n    # Append the computed distance to the distances list\n    distances.append(euclidean_distance(coords_200[xx], stim_coords))\n\n# Convert the list of distances to a NumPy array for easier manipulation\ndistances = np.array(distances)\n\n# Iterate over the indices of the distances array, sorted in ascending order\nfor idx, item in enumerate(np.argsort(distances)):\n    # Check if the network name of the stimulation region is present in the label of the current parcel\n    if stim_net in label_stripped_200[item]:\n        # If the condition is met, assign the index of the current parcel to `parcel2inject`\n        parcel2inject = item\n        # Exit the loop since the desired parcel has been found\n        break\n\n# Extract the absolute values of the EEG data for the specified session\nabs_value = np.abs(all_epo_seeg[list(all_epo_seeg.keys())[ses2use]])\n\n# Normalize each time series by subtracting its mean\nfor xx in range(abs_value.shape[0]):\n    abs_value[xx, :] = abs_value[xx, :] - np.mean(abs_value[xx, :])\n\n# Take the absolute value of the normalized data\nabs_value = np.abs(abs_value)\n\n# Find the starting and ending points around the maximum value in the data\n# Get the index of the maximum value along the time axis\nstarting_point = np.where(abs_value == abs_value.max())[1][0] - 10\nending_point = np.where(abs_value == abs_value.max())[1][0] + 10\n\n# Compute the maximum, mean, and standard deviation of the data within the range around the maximum\nmax_value = np.max(abs_value[:, starting_point:ending_point])\nmean = np.mean(abs_value[:, starting_point:ending_point])\nstd = np.std(abs_value[:, starting_point:ending_point])\n\n# Define a threshold as mean + 4 times the standard deviation\nthr = mean + (4 * std)\n\n# Count the number of unique regions affected by the threshold\nnumber_of_region_affected = np.unique(np.where(abs_value > thr)[0]).shape[0]\n\nimg = nib.load(data_folder + '/calculate-distance/calculate_distance/example/mri/example_Schaefer2018_200Parcels_7Networks_rewritten.nii')\n\n# Get the shape and affine matrix of the image\nshape, affine = img.shape[:3], img.affine\n\n# Create a meshgrid of voxel coordinates\ncoords = np.array(np.meshgrid(*(range(i) for i in shape), indexing='ij'))\n\n# Rearrange the coordinates array to have the correct shape\ncoords = np.rollaxis(coords, 0, len(shape) + 1)\n\n# Apply the affine transformation to get the coordinates in millimeters\nmm_coords = nib.affines.apply_affine(affine, coords)\n\n# Initialize an array to store the coordinates of the 200 parcels\nsub_coords = np.zeros((3, 200))\n\n# Loop over each parcel (1 to 200)\nfor xx in range(1, 201):\n    # Find the voxel coordinates where the parcel value equals the current parcel number\n    vox_x, vox_y, vox_z = np.where(img.get_fdata() == xx)\n\n    # Calculate the mean coordinates in millimeters for the current parcel\n    sub_coords[:, xx - 1] = np.mean(mm_coords[vox_x, vox_y, vox_z], axis=0)\n\n\n\n# Initialize an empty list to store distances\ndistances = []\n\n# Compute the Euclidean distance between each coordinate in the 200-parcel atlas and the coordinate of the parcel to inject\nfor xx in range(coords_200.shape[0]):\n    distances.append(euclidean_distance(sub_coords[:,xx], sub_coords[:,parcel2inject]))\n\n# Convert the list of distances to a NumPy array for further processing\ndistances = np.array(distances)\n\n# Find the indices of the closest parcels to inject, based on the number of affected regions\ninject_stimulus = np.argsort(distances)[:number_of_region_affected]\n\n# Compute stimulus weights based on the distances\n# Adjust distances to a scale of 0 to 1 and calculate the values for the stimulus weights\nvalues = (np.max(distances[inject_stimulus] / 10) + 0.5) - (distances[inject_stimulus] / 10)\n\n# Initialize an array for stimulus weights with zeros\nstim_weights_thr = np.zeros((len(label)))\n\n# Assign the computed values to the stimulus weights for the selected parcels\nstim_weights_thr[inject_stimulus] = values\n\nold_path = data_folder + \"/anatomical/anatomical/example-bem\"\nnew_path = data_folder + \"/anatomical/anatomical/example-bem.fif\" # CS\n\nif not os.path.exists(new_path):\n    os.rename(old_path, new_path)\n    print(f\"Renamed {old_path} to {new_path}\")\n\n# File paths for transformation, source space, and BEM files\ntrans = data_folder + '/anatomical/anatomical/example-trans.fif'\nsrc = data_folder + '/anatomical/anatomical/example-src.fif'\n#bem = 'anatomical/example-bem'\nbem = data_folder + '/anatomical/anatomical/example-bem.fif'\n\n# Create a forward solution using the provided transformation, source space, and BEM files\n# Only EEG is used here; MEG is disabled\nfwd = mne.make_forward_solution(epo_eeg.info, trans=trans, src=src, bem=bem,\n                                meg=False, eeg=True, mindist=5.0, n_jobs=2,\n                                verbose=False)\n\n# Extract the leadfield matrix from the forward solution\nleadfield = fwd['sol']['data']\n\n# Convert the forward solution to a fixed orientation with surface orientation\nfwd_fixed = mne.convert_forward_solution(fwd, surf_ori=True, force_fixed=True,\n                                         use_cps=True)\n# Update the leadfield matrix to use the fixed orientation\nleadfield = fwd_fixed['sol']['data']\n\n# Read the source spaces from the source space file\nsrc = mne.read_source_spaces(src, verbose=False)\n\n# Extract vertex indices for each hemisphere from the forward solution\nvertices = [src_hemi['vertno'] for src_hemi in fwd_fixed['src']]\n\n# Read annotation files for left and right hemispheres\nlh_vertices = nibabel.freesurfer.io.read_annot(data_folder +'/anatomical/anatomical/lh.Schaefer2018_200Parcels_7Networks_order.annot')[0]\nrh_vertices = nibabel.freesurfer.io.read_annot(data_folder +'/anatomical/anatomical/rh.Schaefer2018_200Parcels_7Networks_order.annot')[0]\n\n# Extract vertices corresponding to the parcels from the annotation files\n# Add 100 to right hemisphere vertices to adjust for parcel numbering\nlh_vertices_thr = lh_vertices[vertices[0]]\nrh_vertices_thr = rh_vertices[vertices[1]] + 100\n# Combine left and right hemisphere vertices into a single array\nvertices_thr = np.concatenate([lh_vertices_thr, rh_vertices_thr])\n\n# Initialize a new leadfield matrix with dimensions adjusted for the number of parcels\nnew_leadfield = np.zeros((leadfield.shape[0], np.unique(vertices_thr).shape[0] - 1))\n\n# Compute the average leadfield for parcels in the range 1-100\nfor parcel in range(1, 101):\n    new_leadfield[:, parcel - 1] = np.mean(leadfield[:, np.where(vertices_thr == parcel)[0]], axis=1)\n\n# Compute the average leadfield for parcels in the range 101-200\nfor parcel in range(101, 201):\n    new_leadfield[:, parcel - 1] = np.mean(leadfield[:, np.where(vertices_thr == parcel)[0]], axis=1)\n\n# Load structural connectivity data from a CSV file\nsc_file = 'https://raw.githubusercontent.com/GriffithsLab/PyTepFit/main/data/Schaefer2018_200Parcels_7Networks_count.csv'\nsc_df = pd.read_csv(sc_file, header=None, sep=' ')\nsc = sc_df.values\n\n# Download distance data from Google Drive\ndist_file = \"https://drive.google.com/uc?export=download&id=1EzJNFckal6n4uXMY3h31Wtd9aqsCmgGc\"\nresponse = requests.get(dist_file)\n\n# Save the downloaded distance data to a CSV file\nwith open('Schaefer2018_200Parcels_7Networks_distance.csv', 'wb') as f:\n    f.write(response.content)\n\n# Load the distance data from the saved CSV file\ndist_df = pd.read_csv('Schaefer2018_200Parcels_7Networks_distance.csv', header=None, sep=' ')\ndist = dist_df.values\n\n# Apply log transformation and normalization to the structural connectivity matrix\nsc = np.log1p(sc) / np.linalg.norm(np.log1p(sc))\n\n# Initialize the stimulus weights for further processing\nki0 = stim_weights_thr[:, np.newaxis]\n\n# Extract and normalize EEG data from the evoked response\neeg_data = evoked.data\neeg_data = eeg_data[:, 200:600].T / (np.abs(eeg_data)).max() * 2\n\n# Define model parameters\nnode_size = sc.shape[0]\noutput_size = eeg_data.shape[1]\nbatch_size = 20\nstep_size = 0.0001\npop_size = 3\nnum_epochs = 150\ntr = 0.001\nstate_size = 6\nbase_batch_num = 20\ntime_dim = 400\nhidden_size = int(tr / step_size)\nTPperWindow = batch_size\n\n# Prepare the data structure for the model\ndata_mean = dataloader(eeg_data - eeg_data.mean(1)[:, np.newaxis], num_epochs, batch_size)\n\n# Initialize the leadfield matrix for the model\nlm = new_leadfield.copy() / 10\n\n# Initialize random values for the leadfield matrix\nlm_v = 0.01 * np.random.randn(output_size, 200)\n\n\"\"\"params = ParamsJR(A = par(3.25), a= par(100,100, 2, True), B = par(22), b = par(50, 50, 1, True), \\\n              g=par(200), g_f=par(10), g_b=par(10), \\\n              c1 = par(135, 135, 1, True), c2 = par(135*0.8, 135*0.8, 1, True), \\\n              c3 = par(135*0.25, 135*0.25, 1, True), c4 = par(135*0.25, 135*0.25, 1, True),\\\n              std_in= par(np.log(1.1),np.log(1.1), 0.1, True, True), vmax= par(5), v0=par(6), r=par(0.56), \\\n                  y0=par(-2, -2, 0.3, True),\\\n              mu = par(1,1, 0.1, True), k = par(10,10, .2, True),\\\n                  Mr0 = par(0),\\\n                  Er0 = par(0), Ir0 = par(0),\\\n              cy0 = par(1,1,0.1,True), ki=par(ki0), \\\n              lm=par(lm, lm, .1 * np.ones((output_size, node_size))+lm_v, True))\n\nmodel = RNNJANSEN(params, node_size=node_size, TRs_per_window=TPperWindow, step_size=step_size, output_size=output_size, tr=tr, sc=sc, lm=lm, dist=dist, use_fit_gains=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "create objective function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ObjFun = CostsJR(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "call model fit\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "F = Model_fitting(model, ObjFun)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "u = np.zeros((node_size,hidden_size,time_dim, pop_size))\nu[:,:,65:75,0]= 2000\nF.train(u = u, empRec = data_mean, num_epochs = num_epochs, TPperWindow = TPperWindow,  warmupWindow=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation (with 20 window for warmup)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "F.evaluate(u = u, empRec = data_mean, TPperWindow = TPperWindow, base_window_num = 100)\"\"\"\n\n# @title 150 training\n\n\"\"\"from google.colab import drive\ndrive.mount('/content/drive')\nsave_path = '/content/drive/MyDrive/ClaireShao_WhoBPyT_Replications_Project/Paper 2- Momi_et_al_2025/training_result_momi_2025.pkl'\n\nwith open(save_path, 'wb') as f:\n    pickle.dump(F, f)\"\"\"\n\nparams = ParamsJR(A = par(3.25), a= par(100,100, 2, True), B = par(22), b = par(50, 50, 1, True), \\\n              g=par(200), g_f=par(10), g_b=par(10), \\\n              c1 = par(135, 135, 1, True), c2 = par(135*0.8, 135*0.8, 1, True), \\\n              c3 = par(135*0.25, 135*0.25, 1, True), c4 = par(135*0.25, 135*0.25, 1, True),\\\n              std_in= par(np.log(1.1),np.log(1.1), 0.1, True, True), vmax= par(5), v0=par(6), r=par(0.56), \\\n                  y0=par(-2, -2, 0.3, True),\\\n              mu = par(1,1, 0.1, True), k = par(10,10, .2, True),\\\n                  Mr0 = par(0),\\\n                  Er0 = par(0), Ir0 = par(0),\\\n              cy0 = par(1,1,0.1,True), ki=par(ki0), \\\n              lm=par(lm, lm, .1 * np.ones((output_size, node_size))+lm_v, True))\n\nmodel = RNNJANSEN(params, node_size=node_size, TRs_per_window=TPperWindow, step_size=step_size, output_size=output_size, tr=tr, sc=sc, lm=lm, dist=dist, use_fit_gains=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "create objective function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ObjFun = CostsJR(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "call model fit\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "F = Model_fitting(model, ObjFun)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Training\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "u = np.zeros((node_size,hidden_size,time_dim, pop_size))\nu[:,:,65:75,0]= 2000\nF.train(u = u, empRec = data_mean, num_epochs = 2, TPperWindow = TPperWindow,  warmupWindow=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation (with 20 window for warmup)\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "F.evaluate(u = u, empRec = data_mean, TPperWindow = TPperWindow, base_window_num = 100)\n# @title 2 epoch\n\n\"\"\"load_path = '/content/drive/MyDrive/ClaireShao_WhoBPyT_Replications_Project/Paper 2- Momi_et_al_2025/training_result_momi_2025.pkl'\n\nwith open(load_path, 'rb') as f:\n    F = pickle.load(f)\"\"\"\n\ntime_start = np.where(evoked.times==-0.1)[0][0]\ntime_end = np.where(evoked.times==0.3)[0][0]\n\nch, peak_locs1 = evoked.get_peak(ch_type='eeg', tmin=-0.05, tmax=0.015);\nch, peak_locs2 = evoked.get_peak(ch_type='eeg', tmin=0.015, tmax=0.03);\nch, peak_locs3 = evoked.get_peak(ch_type='eeg', tmin=0.03, tmax=0.04);\nch, peak_locs4 = evoked.get_peak(ch_type='eeg', tmin=0.04, tmax=0.06);\nch, peak_locs5 = evoked.get_peak(ch_type='eeg', tmin=0.08, tmax=0.12);\nch, peak_locs6 = evoked.get_peak(ch_type='eeg', tmin=0.12, tmax=0.2);\n\nts_args = dict(xlim=[-0.1,0.3]) #Time to plot\n\ntimes = [peak_locs1, peak_locs2, peak_locs3, peak_locs4, peak_locs5, peak_locs6]\n\nevoked_joint_st = evoked.plot_joint(ts_args=ts_args, times=times);\n\n\nsimulated_EEG_st = evoked.copy()\n\nsimulated_EEG_st.data[:,time_start:time_end] = F.trainingStats.outputs['eeg_testing']\n\nsimulated_joint_st = simulated_EEG_st.plot_joint(ts_args=ts_args, times=times)\n\n\"\"\"## Virtual_dissection\"\"\"\n\n\n\nurl = 'https://github.com/Davi1990/DissNet/raw/main/examples/network_colour.xlsx'\ncolour = pd.read_excel(url, header=None)[4]\ntemplate_eeg = mne.read_epochs(data_folder + '/virtual-dissection/eeg_template.fif', verbose=False)\n\nmodel_results =np.load(data_folder + '/virtual-dissection/model_results.npy', allow_pickle=True).item()\n\nwith open(data_folder + '/empirical-data/dist_Schaefer_1000parcels_7net.pkl', 'rb') as handle:\n    stim_region = pickle.load(handle)\nstim_region = stim_region['stim_region']\n\nnetworks = ['Vis', 'SomMot', 'DorsAttn', 'SalVentAttn', 'Limbic', 'Cont', 'Default']\n# Create a dictionary to store the network indices\nstim_network_indices = {network: [] for network in networks}\nfor i, label in enumerate(stim_region):\n    # Iterate over each network\n    for network in networks:\n        if network in label:\n            stim_network_indices[network].append(i)\n            break\n\n# Calculate the number of subplots needed\nnum_plots = len(networks)\nnum_rows = 3\nnum_cols = (num_plots + num_rows - 1) // num_rows\n\n# Set the size of the figure\nfig_width = 12  # Adjust as needed\nfig_height = 10  # Adjust as needed\nplt.figure(figsize=(fig_width, fig_height))\n\n# Loop over networks\nfor i, network in enumerate(networks):\n    # Create subplots\n    plt.subplot(num_rows, num_cols, i + 1)\n\n    # Plot standard deviation of EEG test data\n    plt.plot(template_eeg.times[200:600], np.mean(np.std(model_results['eeg_test'][stim_network_indices[network]], axis=1), axis=0) - .1,\n             color=colour[i], linestyle='--', label='eeg_test')\n\n    # Plot standard deviation of EEG test lesion data\n    plt.plot(template_eeg.times[200:600], np.mean(np.std(model_results['eeg_test_lesion'][stim_network_indices[network]], axis=1), axis=0) -.1,\n             color=colour[i], label='eeg_test_lesion')\n\n    plt.ylim(0, 0.4)\n\n    # Add title, labels, and legend\n    plt.title(f'Network: {network}')\n    plt.xlabel('Time')\n    plt.ylabel('GMFA')\n\n\n# Adjust layout to prevent overlap\nplt.tight_layout()\n\n\n# Show the plot\nplt.show()\n\nwindows = 3\n\nAUC_original = np.zeros((3,model_results['eeg_test_lesion'].shape[0]))\nAUC_simulation = np.zeros((3,model_results['eeg_test_lesion'].shape[0]))\n\nfor ses in range(model_results['eeg_test_lesion'].shape[0]):\n\n    original_ts = np.std(model_results['eeg_test'][ses], axis=0)\n\n    AUC_original[0, ses] = np.trapz(original_ts[100:137] - np.mean(original_ts[:100]), dx=5)\n    AUC_original[1, ses] = np.trapz(original_ts[137:178] - np.mean(original_ts[:100]), dx=5)\n    AUC_original[2, ses] = np.trapz(original_ts[178:397] - np.mean(original_ts[:100]), dx=5)\n\n\n    lesion_ts = np.std(model_results['eeg_test_lesion'][ses], axis=0)\n\n    AUC_simulation[0, ses] = np.trapz(lesion_ts[100:137] - np.mean(lesion_ts[:100]), dx=5)\n    AUC_simulation[1, ses] = np.trapz(lesion_ts[137:178] - np.mean(lesion_ts[:100]), dx=5)\n    AUC_simulation[2, ses] = np.trapz(lesion_ts[178:397] - np.mean(lesion_ts[:100]), dx=5)\n\n\nAUC_original[0,:] = AUC_original[0,:] / 37\nAUC_original[1,:] = AUC_original[1,:] / 45\nAUC_original[2,:] = AUC_original[2,:] / 322\n\nAUC_simulation[0,:] = AUC_simulation[0,:] / 37\nAUC_simulation[1,:] = AUC_simulation[1,:] / 45\nAUC_simulation[2,:] = AUC_simulation[2,:] / 322\n\n\nnet_AUC_orig = {}\nnet_AUC_lesion = {}\n\nfor network in networks:\n    net_AUC_orig[network] = AUC_original[:,stim_network_indices[network]]\n    net_AUC_lesion[network] = AUC_simulation[:,stim_network_indices[network]]\n\n\nAUC_averages_original = np.zeros((len(networks), windows))\nAUC_averages_lesion = np.zeros((len(networks), windows))\n\nfor idx, key in enumerate(net_AUC_orig.keys()):\n    AUC_averages_original[idx, :] = np.mean(net_AUC_orig[key], axis=1)\n    AUC_averages_lesion[idx, :] = np.mean(net_AUC_lesion[key], axis=1)\n\n\nAUC_averages_lesion = AUC_averages_lesion #*100000\nAUC_averages_original = AUC_averages_original #*100000\n\n\n\n\n# Create the figure and subplots\nfig, axs = plt.subplots(1, 3, figsize=(13, 6))  # 1 row, 3 columns\n\n# Plotting the data on each subplot\nfor i in range(3):\n    axs[i].bar(range(AUC_averages_original.shape[0]), 5 * (AUC_averages_lesion[:, i] - AUC_averages_original[:, i]), color=colour)\n    axs[i].set_xticks(range(AUC_averages_original.shape[0]))\n    axs[i].set_xticklabels(networks, rotation=45)\n    axs[i].set_xlabel('Networks')\n    axs[i].set_ylabel('AUC')\n    axs[i].set_title(f'Response {i+1}')\n    axs[i].set_ylim(-1.5, 1)  # Adjust the y-axis limits as needed\n\n# Adjust layout\nplt.tight_layout()\n\n\nplt.show()\n\nall_lesioned_gfma = np.zeros((model_results['eeg_test'].shape[0], model_results['eeg_test'].shape[2]))\nall_original_gfma = np.zeros((model_results['eeg_test'].shape[0], model_results['eeg_test'].shape[2]))\n\n\n\nfor ses in range(model_results['I_test_lesion'].shape[0]):\n        ts2use = (model_results['eeg_test_lesion'] )[ses,:,:]\n        all_lesioned_gfma[ses,:] =  np.std(ts2use, axis=0)\n        all_original_gfma[ses,:] =  np.std((model_results['eeg_test'] )[ses,:,:]  , axis=0)\n\n\nnet_lesioned_gfma = {}\n\nfor network in networks:\n    net_lesioned_gfma[network] = all_lesioned_gfma[stim_network_indices[network]]\n\n\naverages_lesioned = []\n\nfor key, value in net_lesioned_gfma.items():\n    average_lesioned = sum(value) / len(value)\n    averages_lesioned.append(average_lesioned)\n\naverages_lesioned = np.array(averages_lesioned)\n\n# Download the file from the GitHub URL\nurl = 'https://github.com/Davi1990/DissNet/raw/main/examples/network_colour.xlsx'\ncolour = pd.read_excel(url, header=None)[4]\n\n\nfig = plt.figure(figsize=(20, 6))\n\n# Plot the data\nfor net in range(len(networks)):\n    plt.plot(template_eeg.times[200:600], averages_lesioned[net, :], colour[net], linewidth=5)\n\n    #plt.plot(a.times[200:600], averages_lesioned[net, :]- np.mean(averages_lesioned[net, :100]), colour[net], linewidth=5)\n    plt.ylim([0.08,0.45])\n# Display the plot\n\nplt.show()\n\n\"\"\"## Applying_virtual_dissection\"\"\"\n\n\n\n# @title Install dependencies\n\n# URL of the CSV file containing centroid coordinates for Schaefer2018 atlas\nurl = 'https://raw.githubusercontent.com/ThomasYeoLab/CBIG/master/stable_projects/brain_parcellation/Schaefer2018_LocalGlobal/Parcellations/MNI/Centroid_coordinates/Schaefer2018_200Parcels_7Networks_order_FSLMNI152_2mm.Centroid_RAS.csv'\n\n# Read the CSV file into a DataFrame\natlas = pd.read_csv(url)\n\n# Extract the 'ROI Name' column from the DataFrame\nlabel = atlas['ROI Name']\n\n# Create a list to store stripped labels\nlabel_stripped = []\n\n# Strip '7Networks_' from each label and append to the list\nfor xx in range(len(label)):\n    label_stripped.append(label[xx].replace('7Networks_', ''))\n\n# Define the list of network names\nnetworks = ['Vis', 'SomMot', 'DorsAttn', 'SalVentAttn', 'Limbic', 'Cont', 'Default']\n\n# Create a dictionary to store the network indices\nnetwork_indices = {network: [] for network in networks}\n\n# Iterate over each stripped label\nfor i, label in enumerate(label_stripped):\n    # Iterate over each network\n    for network in networks:\n        if network in label:\n            # Append the index to the corresponding network's list in the dictionary\n            network_indices[network].append(i)\n            break\n\n# Define the stimulated network\nsti_net = 'Default'\n\n# Convert the list of indices for the stimulated network to a numpy array\nnetwork_indices_arr = np.array(network_indices[sti_net])\n\n# Get the indices that do not belong to the stimulated network\ndiff = np.array(list(set(np.arange(200)) - set(network_indices_arr)))\n\n#already trained file\nfit_file = data_folder + '/example-fittingresults/example-fittingresults.pkl'\n\n\n# Define model parameters\nstate_lb=-0.2\nstate_ub=0.2\ndelays_max = 500\nwhen_damage = 80\nnode_size = 200\nbatch_size = 20\nstep_size = 0.0001\npop_size=3\nnum_epochs = 150\ntr = 0.001\nstate_size = 2\nbase_batch_num = 20\ntime_dim = 400\nstate_size = 2\nbase_batch_num = 100\nhidden_size = int(tr/step_size)\nTPperWindow=batch_size\nnode_size = 200\nstate_size = 2\ntransient_num = 10\npop_size = 3\n\ntransient_num = 10\nfinal_ouput_P = []\nfinal_ouput_E = []\nfinal_ouput_I = []\nfinal_ouput_eeg = []\n\n# Initialize an empty dictionary to store lesion data\n\"\"\"lesion_data = {}\n\n# Load data from a pickle file\nwith open(fit_file, 'rb') as f:\n    data = pickle.load(f)\n\n# Initialize the state tensor x0 with random values uniformly distributed between state_lb and state_ub\nx0 = torch.tensor(np.random.uniform(state_lb, state_ub,\n              (data.model.node_size, pop_size, data.model.state_size)), dtype=torch.float32)\n\n# Initialize the hemodynamic state tensor he0 with random values uniformly distributed between state_lb and state_ub\nhe0 = torch.tensor(np.random.uniform(state_lb, state_ub,\n               (data.model.node_size, delays_max)), dtype=torch.float32)\n\n# Create an input tensor u with zeros, with dimensions 200x10x80xpop_size\nu = np.zeros((200, 10, when_damage, pop_size))\n\n# Apply a stimulus of 2000 units to a specific time range (65-75ms) for the first population\nu[:, :, 65:75, 0] = 2000\n\n# Create a mask with ones of shape 200x200\nmask = np.ones((200, 200))\n\n# Assign the mask to the model's mask attribute\ndata.model.mask = mask\n\n# Initialize data_mean with ones, with dimensions 1x8x(output_size)x(TRs_per_window)\ndata_mean = np.ones(([1, int(when_damage / TPperWindow), data.model.output_size, data.model.TRs_per_window]))\n\n# Evaluate the model with the given input tensor u, empirical data data_mean, and initial states x0 and he0\ndata.evaluate(u=u, empRec=data_mean, TPperWindow=data.model.TRs_per_window, X=x0, hE=he0, base_window_num=100)\n\n# Append the training states for P, E, I, and EEG to their respective final output lists\nfinal_ouput_P.append(data.trainingStats.states['testing'][:, 0, 0])\nfinal_ouput_E.append(data.trainingStats.states['testing'][:, 1, 0])\nfinal_ouput_I.append(data.trainingStats.states['testing'][:, 2, 0])\nfinal_ouput_eeg.append(data.trainingStats.outputs['eeg_testing'])\n\n# Update x0 with the last state of the trainingStats testing states\nx0 = torch.tensor(np.array(data.trainingStats.states['testing'][:, :, :, -1]))\n\n# Update he0 by concatenating the reversed first state of the testing states and the remaining part of the original he0\nhe0 = torch.tensor(np.concatenate(\n    [data.trainingStats.states['testing'][:, 0, 0][:, ::-1],\n     he0.detach().numpy()[:, :500 - data.trainingStats.states['testing'][:, 0, 0].shape[1]]], axis=1))\n\n# Load data from a pickle file\nwith open(fit_file, 'rb') as f:\n    data = pickle.load(f)\n\n# Create a mask with ones of shape 200x200\nmask = np.ones((200, 200))\n\n# Set the mask elements corresponding to network_indices_arr and diff to 0\nmask[np.ix_(network_indices_arr, diff)] = 0\n\n# Create an input tensor u with zeros, with dimensions 200x10x(320)xpop_size\nu = np.zeros((200, 10, int(400 - when_damage), pop_size))\n\n# Initialize data_mean with ones, with dimensions 1x16x(output_size)x(TRs_per_window)\ndata_mean = np.ones(([1, int((400 - when_damage) / TPperWindow), data.model.output_size, data.model.TRs_per_window]))\n\n# Evaluate the model with the given input tensor u, empirical data data_mean, initial states x0, he0, and mask\ndata.evaluate(u=u, empRec=data_mean, X=x0, hE=he0, TPperWindow=data.model.TRs_per_window, base_window_num=0, mask=mask)\n\n# Append the training states for P, E, I, and EEG to their respective final output lists\nfinal_ouput_P.append(data.trainingStats.states['testing'][:, 0, 0])\nfinal_ouput_E.append(data.trainingStats.states['testing'][:, 1, 0])\nfinal_ouput_I.append(data.trainingStats.states['testing'][:, 2, 0])\nfinal_ouput_eeg.append(data.trainingStats.outputs['eeg_testing'])\n\n# Concatenate the first and second elements of the final output lists along axis 1\nnew_P = np.concatenate((final_ouput_P[0], final_ouput_P[1]), axis=1)\nnew_E = np.concatenate((final_ouput_E[0], final_ouput_E[1]), axis=1)\nnew_I = np.concatenate((final_ouput_I[0], final_ouput_I[1]), axis=1)\nnew_eeg = np.concatenate((final_ouput_eeg[0], final_ouput_eeg[1]), axis=1)\n\n# Read the epoched data from a .fif file\nepoched = mne.read_epochs(data_folder + '/empirical-data/example_epoched.fif', verbose=False)\n\n# Compute the average evoked response from the epoched data\nevoked = epoched.average()\n\n# Find the index corresponding to the time -0.1 seconds\ntime_start = np.where(evoked.times == -0.1)[0][0]\n\n# Find the index corresponding to the time 0.3 seconds\ntime_end = np.where(evoked.times == 0.3)[0][0]\n\n# Load data from a pickle file\nwith open(fit_file, 'rb') as f:\n    data = pickle.load(f)\n\n# Create a copy of the evoked data for simulation\nsimulation = evoked.copy()\n\n# Replace the simulation data in the time range from time_start to time_end with the EEG testing data\nsimulation.data[:, time_start:time_end] = data.trainingStats.outputs['eeg_testing']\n\n# Find peak locations in specified time windows and store them\nch, peak_locs1 = simulation.get_peak(ch_type='eeg', tmin=-0.05, tmax=0.015)\nch, peak_locs2 = simulation.get_peak(ch_type='eeg', tmin=0.015, tmax=0.03)\nch, peak_locs3 = simulation.get_peak(ch_type='eeg', tmin=0.03, tmax=0.04)\nch, peak_locs4 = simulation.get_peak(ch_type='eeg', tmin=0.04, tmax=0.06)\nch, peak_locs5 = simulation.get_peak(ch_type='eeg', tmin=0.08, tmax=0.12)\nch, peak_locs6 = simulation.get_peak(ch_type='eeg', tmin=0.12, tmax=0.2)\n\n# Set the y-axis limits for plotting\nymin = -1.8e6\nymax = 1.8e6\n\n# Define plotting arguments with x and y limits\nts_args = dict(xlim=[-0.1, 0.3], ylim=dict(eeg=[ymin, ymax]))\n\n# List of peak locations to highlight in the plot\ntimes = [peak_locs1, peak_locs2, peak_locs3, peak_locs4, peak_locs5, peak_locs6]\n\n# Plot the simulation data with specified arguments and peak times\nsimulation_st = simulation.plot_joint(ts_args=ts_args, times=times)\n\n# Create a copy of the evoked data for lesion simulation\nlesion = evoked.copy()\n\n# Replace the lesion data in the time range from time_start to time_end with the new EEG data\nlesion.data[:, time_start:time_end] = new_eeg\n\n# Plot the lesion data with specified arguments and peak times\nlesion_st = lesion.plot_joint(ts_args=ts_args, times=times)\n\n# Plot the standard deviation of the EEG testing data across the time dimension\nplt.plot(np.std(data.trainingStats.outputs['eeg_testing'], axis=0), label='Intact Structural Connectome')\n\n# Plot the standard deviation of the new EEG data (after virtual dissection) across the time dimension\nplt.plot(np.std(new_eeg, axis=0), label='Virtual Dissection', linestyle='--')\n\n# Add labels and title\nplt.xlabel('Time Points')\nplt.ylabel('Global Mean Field Power')\nplt.title('Comparison of GMFP: Intact vs. Virtual Dissection')\nplt.legend()\n\n# Show the plot\nplt.show()\"\"\"\n\n# @title difference"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.22"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}